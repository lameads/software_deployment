NOTE: This guide works for all series of machine: both 23 and 24.

NOTE: We had issues with the wifi NICs not getting any reads from traffic when deployed. I am thinking it has to do with the avahi-deamon interfering and possibly putting the NICs back into managed mode. The following link discusses how to stop and disable the avahi-daemon: https://askubuntu.com/questions/205937/how-can-i-disable-avahi-daemon
Below is what I believe to be the steps necessary for stopping it completely:
You need to mask the socket to prevent applications starting avahi-daemon:
systemctl mask avahi-daemon.socket
systemctl disable avahi-daemon
systemctl stop avahi-daemon

The following also may be of interest for disabling and keeping the avahi-daemon from trying to respawn:
You might not need to stop it from starting up when PC boots, but maybe prevent it from respawn as it does, when you kill it.
So, just comment out the respawn in /etc/init/avahi-daemon.conf
Killing that process always failed, so couldn't get airmon-ng check to pass. Commenting out the respawning, it kills and never comes back hunting me.

FOR EXTERNAL CLIENTS' MACHINES: 
When delaing with a client's machine where we must create the guard user from the command line and it is not the first user, we need to create the .ssh directory in the guard user's home directory:
1. Either create the .ssh folder inside the user's main directory and run chmod 700 .ssh, or run ssh keygen.
2. Create a keys directory within the .ssh directory. 
3. Inside the keys directory place the sgs_stats_api1.pem key, then chmod it with 400.
4. To create a new user do sudo su as the default user, then do sudo adduser guard and provide the desired password. We will then run sudo visudo (or just visudo as root) and add the guard user as described below in step 3 of the PRE-SETUP section. Then we must change the password for the default user. Run exit to get back to default user, then run passwd. Change the password to whatever you want. Be sure that the new password is not the same as that of the guard user. Then we can exit and log in as guard to the same machine via ssh and carry on as normal. Or just run su guard to switch to the guard user from root.

******** NO LONGER NEEDED ********
The next step to ensure that the client-created user (for example drop-data-1004) cannot get root privileges and see the files within our guard user that we create is to remove it from the sudoers group altogether. First check the sudoers file with sudo visudo as the guard user. Look for any explicit granting of sudo privileges to the client-created user and either comment them out or remove them. Be sure that the guard user is added as a sudoer before you remove the client-created user from root privileges. Otherwise no users will be able to get root privileges. See step 3 in the PRE-SETUP section. DO THIS BEFORE YOU REVOKE PRIVILEGES FROM THE CLIENT-CREATED USER!!!
Finally, run sudo deluser <client-created username> sudo. For example, for the user "drop-data-1004", we would call:
sudo deluser drop-data-1004 sudo
This will remove the drop-data-1004 user from the sudo users group which will make it so they can not run sudo commands and they cannot see or access the files stored within the guard user's directory.
**********************************

PRE-SETUP:
1. Install Ubuntu 24.04 on the machine.
2. Create the guard user with a password.
3. Use sudo visudo to modify permissions so that the guard user can run as root and can run sudo commands without requiring a password (you may have to run sudo su in order to get to root when doing this).
    The following steps are used to accomplish this:
        - sudo su
        - sudo visudo
        - Add guard ALL=(ALL:ALL)ALL below the line that specifies root ALL=(ALL:ALL)ALL
        - At the very bottom of the file user the line that says "@includedir etc/sudoers.d", append a new line that   says "guard ALL=(ALL)NOPASSWD:ALL"
        - Save and exit the file.
4. Install git using sudo apt install git
5. NOTE: Current access token is: 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
6. Exit out of the root user back to the guard user. You can just type exit into the terminal to accomplish this.
7. Clone the safeguard_deployments repository using the access token from step 5 into the home directory of the guard user (/home/guard)
8. cd into the safeguard_deployments directory and move the setup_master.sh script into the guard user's home directory: mv ./setup_master.sh /home/guard
9. Make the setup_master.sh script executable: chmod +x setup_master.sh
10. Run the script: ./setup_master.sh
11. This script will install all dependencies for the bluetooth sniffer, wifi sniffer, lpr server, and corresponding log files, and will also install the drivers necessary to run the WiFi adapters on Ubuntu 24.04
12. Update the box_id.txt file to contain the proper box ID for the location that the machine will be deployed at. Also add the name of the location to the location_config.txt file. These files are both in the /home/guard directory.
13. Add the sgs_stats_api1.pem key:
    - cd into the .ssh folder in the home directory: cd /home/guard/.ssh
    - Create a directory called "keys": mkdir keys
    - cd into the safeguard_deployments directory. Inside this directory there is a file called sgs_stats_api1.pem. 
      Move this file into /home/guard/.ssh/keys: mv ./sgs_stats_api1.pem /home/guard/.ssh/keys
    - Give 400 level permissions to the key: chmod 400 sgs_stats_api1.pem

TURN OFF AUTOSUSPEND:
1. To be sure that autosuspend is turned off, open the gdm3 config file: sudo nano /etc/gdm3/greeter.dconf-defaults
2. Uncomment the line that says sleep-inactive-ac-timeout and change the value to 0
3. Uncomment the line that says sleep-inactive-ac-type and change the value to 'blank'
4. Save and exit the file. Restart the gdm3 service with sudo systemctl restart gdm3 to make the changes take place on the system. This will cause the computer screen to go blank if you are using the GUI and you will have to log into the guard account again.
5. Ensure that RDP is enabled on the machine so we can remote in with RDP if needed in the future.
    Go to Setting->System
    Click remote desktop
    Click the remote login tab
    Enable Remote Login
    Make username guard
    Make a password that you will remember (you can use the guard password that you created)
    You can click verify encryption but I don't think we need to do that
    Exit out of settings and go back in to verify that all changes were saved
    Note that remote desktop will be accessed via port 3389.
    Desktop sharing is accessed via port 3390 (typically, if remote desktop is enabled. Ubuntu should assign these ports by default).

REBOOT:
1. Reboot the machine for the new driver for the wifi adapters to start working. 
2. Upon restarting, type ip a into the terminal. You should now see (in addition to other interfaces) 8 devices whose names all start with "wlx".

WIFI SNIFFING DEPLOYMENT:
1. Navigate into the safeguard_deployments directory.
2. Move the wifi_sniffer folder out of this directory and into the home directory: mv wifi_sniffer /home/guard
1. Navigate into the wifi_sniffer directory
3. Make send_wifi_stats.sh ,clean_cap.sh, and stop.sh executable: chmod +x <filename>
    - Note that clean_cap.sh is used to clean out the capture folers. This is just a utility script, so use this when you want to clean the folders out without sending files up to the api server.
4. Make the pre-compiled sniff program executable: chmod +x sniff
5. Create a virtual environment that will be used to run the get_nics.py python script: python3 -m venv nic_env
    Activate virtual environment: source nic_env/bin/activate
    Install requirements.txt: pip3 install -r requirements.txt
    Deactivate the virtual environment
6. The C code will take care of creating the capture directories automatically. One capture directory will be created for each NIC used in the capture (up to 8 maximum NICs). You do need to specify the interface names inside of the send_wifi_stats.sh script though. At the bottom of the script, where the sniff program is called, replace the interface name (wlx...) with the interface names on the machine you are deploying. To get interface names, run: ip a
Once the interface names have been copied over, write out and exit the script.
    - NOTE: There should be 8 interfaces. If there aren't, you will need to change the expected_nic_count variable from 8 to the number of wlx... interfaces are found on your machine. The expected_nic_count variable can be found at the top of the send_wifi_stats.sh script.
7. A backups directory will be created automatically by the send_wifi_stats.sh script if it does not already exist.
8. Run a test by calling ./send_wifi_stats.sh. All directories should be created automatically, including the backups directory. 8 capture directories should be created if we are deploying on the typical machine that has 8 NICs connected. Let the program run for a few minutes to be sure everything is functioning properly. Once you are satisfied everything is working, call ./send_wifi_stats again. This should send up the captured files to the api server. Since it is your first time running the script, you will be promted with a question about your .pem key. Type "yes" and hit enter to continue. The program will restart and begin capturing again.
9. Stop the script without sending stats by running the ./stop.sh script.
10. If stop.sh succeeded, there will be no output in the terminal from the script. If it fails, it will tell you (most likely) that no process exists.
11. If you aren't sure whether the program is running or not, you can type the following into the command line and hit enter: top
12. Once stopped, you can clean out the folders by running ./clean_cap.sh. You will also probably want to clear out the backups folder. To do this, run:
    - cd backups
    - sudo rm *.pcap
13. The wifi sniffer should be ready to go at this point.

BLUETOOTH SNIFFING DEPLOYMENT:
1. Navigate into the safeguard_deployments directory.
2. Move the bluetooth_sniffer folder out of this directory and into the home directory: mv bluetooth_sniffer /home/guard
3. Navigate into the bluetooth_sniffer folder: cd /home/guard/bluetooth_sniffer. Make stop.sh and send_bt_stats.sh executable: chmod +x <filename>
4. Run the following command: sudo bluetoothctl
    - This will start the bluetoothctl tool. Now type list
    - The MAC addresses of the connected bluetooth adapters should be listed for you.
    - Copy each of these MAC addresses into the send_bt_stats.sh script on the line that calls "./bt_scanners", in between the ampersand and the "./bt_scanners" call. Separate the MAC addresses with a         space. If one of the MAC addresses begins with 80:, be sure that you put that MAC address SECOND in the list. The reason for this is the MAC address starting with 80 indicates that it is the MAC         address of the on-board bluetooth adapter, and this adapter is not capable of sniffing bluetooth low energy. Note that there should be 2 adapters listed. The MAC address starting with "80" should        only be present on the 23 series boxes. You won't have to worry about it on the 24 series boxes.
    - Save the changes you made to the send_bt_stats.sh file and exit out of the file.
5. The send_bt_stats.sh script should create both capture folders if they do not yet exist, as well as the backups folder if it does not yet exist.
7. Run the send_bt_stats.sh with ./send_bt_stats.sh.
8. Check that the files are being saved.
9. You can try sending stats up to the api server by running ./send_bt_stats.sh again, or you can stop the capture altogether with ./stop.sh
10. Clean outthe hci0_cap_le and hci1_cap_classic directories by navigating into each and running: sudo rm *.csv. You should also probably do the same for the backups folder.
11. At this point the bluetooth sniffer should be ready to go.

LPR DEPLOYMENT:
1. Navigate to the safeguard_deployments directory: cd /home/guard/safeguard_deployments
2. Move the lpr_server folder from safeguard_deployments into the guard user's home directory: mv lpr_server /home/guard
3. Navigate into the lpr_server directory and create a virtual environment for the server to run:
    Try creating the virtual environment with "python3 -m venv venv" (uses venv to create a virtual environment in the current directory called "venv")
    IFF THAT FAILS, you should get a message stating that you must install venv using "sudo apt install python3.12-venv". Run this command to install and try creating the virtual environment again.
4. NOTE: Only do this step if you encounter issues with running the server. The following should NOT be needed: If the following aren't already installed, install the following with: sudo apt install python3-pip python3-dev build-essential libssl-dev libffi-dev python3-setuptools (shouldn't need to do this though). I believe these packages should already be installed, but I included this step in case we get errors later on in the setup process for troubleshooting purposes.
5. Install modules from requirements.txt
    Navigate to the project folder and activate the virtual environment with source ./venv/bin/activate
    run pip3 install -r requirements.txt
    Start the server in development mode by running python3 app.py
6. Ensure that Gunicorn can run the application properly by manually running with Gunicorn: gunicorn --bind 0.0.0.0:5000 wsgi:app. You should be able to navigate to the url and see a method not permitted
    response from the server (the server is only configured to handle POST requests). You can also use postman to test it out as well but this probably isn't necessary.
7. Deactivate the virtual environment on the command line with deactivate
9. Create a unit file for the flask app service called lpr_server.service sudo nano /etc/systemd/system/lpr_server.service In the file, add the following (copy and paste the text between the asterisks if you want):
***********************************************************************************************
[Unit]
Description=Gunicorn instance to serve lpr_server
After=network.target

[Service]
User=guard
Group=www-data
WorkingDirectory=/home/guard/lpr_server
Environment="PATH=/home/guard/lpr_server/venv/bin"
ExecStart=/home/guard/lpr_server/venv/bin/gunicorn --workers 3 --bind 127.0.0.1:7982 wsgi:app

[Install]
WantedBy=multi-user.target
***********************************************************************************************
10. Start and enable the service with the following:
    sudo systemctl start lpr_server
    sudo systemctl enable lpr_server (running enable will make it so the lpr_server service starts up on boot automatically)
11. Check the status of the service with sudo systemctl status lpr_server
12. Configure Nginx to pass the requests to the flask server running on the specified port in the wsgi.py file entry point. To do this we need to create a new server block configuration file in the  
    sites-available directory with the following: sudo nano /etc/nginx/sites-available/lpr_server
    The file should look as follows (copy and paste the text between the asterisks if you want):
***********************************************************************************************
server {
    listen 7985;
    server_name localhost;

    location / {
        include proxy_params;
        proxy_pass http://127.0.0.1:7982;
        # proxy_pass http://unix:home/guard/lpr_server/lpr_server.sock;
    }
}
***********************************************************************************************
13. Test this file for errors with: sudo nginx -t
14. Link the server block config file we just created to the sites-enabled folder with a simlink: sudo ln -s /etc/nginx/sites-available/lpr_server /etc/nginx/sites-enabled
15. Restart nginx: sudo systemctl restart nginx
16. Check the status of nginx by running sudo systemctl status nginx
17. Enable nginx so that it starts the enabled service(s) on boot: sudo systemctl enable nginx
18. Check nginx status and the lpr_server.service status again as well to be sure everything is running properly together.
19. ONLY DO THIS IF ERRORS ARE ENCOUNTERED: Adjust the firewall if necessary (only do this if we need to at a later point I'd say, and only if we run into issues getting the server to work).
20. At this point, the server should be up and running. Be sure that we have called enable on both the wsgi service as well as nginx so that they both start on boot.
21. Make the lpr_send_stats.sh script executable by running chmod +x lpr_send_stats.sh
23. Enter the "backups" directory and run sudo rm *.txt to remove the "dummie.txt" file. This file was placed as a placeholder so we can have the directory stored on github.

VERIFY:
1. Verify that all of the log files were created:
    - cd /var/log
    - ls
    - bt_bash.log, wifi_bash.log, and lpr_bash.log should all be present in this directory
2. Run sudo systemctl status nginx. Check the output to ensure that it is running and it is enabled. Press q to quit if necessary.
3. Run sudo systemctl status lpr_server. Check the output to ensure that it is running and it is enabled. Press q to quit if necessary.

CRONTAB SETUP:
1. Type "crontab -e"
2. A list of text editors will be shown for you to choose from. I would recommend choosing option 1 (nano), as this is the most user-friendly text editor in my opinion.
3. The crontab should open. You can copy the crontab setup below:

*/15 * * * * /usr/bin/flock -n /tmp/send_wifi_stats.lock /home/guard/wifi_sniffer/send_wifi_stats.sh >> /var/log/wifi_bash.log 2>&1
*/18 * * * * /usr/bin/flock -n /tmp/send_bt_stats.lock /home/guard/bluetooth_sniffer/send_bt_stats.sh >> /var/log/bt_bash.log 2>&1
* * * * * /usr/bin/flock -n /tmp/lpr_send_stats.lock /home/guard/lpr_server/lpr_send_stats.sh >> /var/log/lpr_bash.log 2>&1

10 0 * * * find /home/guard/lpr_server/backups/* -name "*.csv" -type f -mtime +3 -exec rm {} \;
11 0 * * * find /home/guard/wifi_sniffer/backups/* -name "*.pcap" -type f -mtime +7 -exec rm {} \;
12 0 * * * find /home/guard/bluetooth_sniffer/backups/* -name "*.csv" -type f -mtime +7 -exec rm {} \;

2 0 1 * * cat /dev/null > /var/log/wifi_bash.log
2 0 1 * * cat /dev/null > /var/log/bt_bash.log
2 0 1 * * cat /dev/null > /var/log/lpr_bash.log



At this point the machine should be setup in terms of capture software. The vpn configuration file will still need to be installed though.
